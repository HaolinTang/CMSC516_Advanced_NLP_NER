{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NER_spaCy\n",
        "# Adapted from Medium (https://towardsdatascience.com/a-review-of-named-entity-recognition-ner-using-automatic-summarization-of-resumes-5248a75de175).\n",
        "# Implemented  by Haolin Tang\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load dataset\n",
        "dataturks_JSON_FilePath = '/content/drive/My Drive/Entity Recognition in Resumes.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goV1RtbFi5eb",
        "outputId": "ddcab4d3-cab5-4f31-8581-0b830a909630"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "GHqMGD8o2rdF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "aVGr1lrohQEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
        "    training_data = []\n",
        "    lines=[]\n",
        "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        data = json.loads(line)\n",
        "        text = data['content'].replace(\"\\n\", \" \")\n",
        "        entities = []\n",
        "        data_annotations = data['annotation']\n",
        "        if data_annotations is not None:\n",
        "            for annotation in data_annotations:\n",
        "                #only a single point in text annotation.\n",
        "                point = annotation['points'][0]\n",
        "                labels = annotation['label']\n",
        "                # handle both list of labels or a single label.\n",
        "                if not isinstance(labels, list):\n",
        "                    labels = [labels]\n",
        "\n",
        "                for label in labels:\n",
        "                    point_start = point['start']\n",
        "                    point_end = point['end']\n",
        "                    point_text = point['text']\n",
        "\n",
        "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
        "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
        "                    if lstrip_diff != 0:\n",
        "                        point_start = point_start + lstrip_diff\n",
        "                    if rstrip_diff != 0:\n",
        "                        point_end = point_end - rstrip_diff\n",
        "                    entities.append((point_start, point_end + 1 , label))\n",
        "        training_data.append((text, {\"entities\" : entities}))\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "_kMCMGnQ2wN_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim_entity_spans(data: list) -> list:\n",
        "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
        "\n",
        "    Args:\n",
        "        data (list): The data to be cleaned in spaCy JSON format.\n",
        "\n",
        "    Returns:\n",
        "        list: The cleaned data.\n",
        "    \"\"\"\n",
        "    invalid_span_tokens = re.compile(r'\\s')\n",
        "\n",
        "    cleaned_data = []\n",
        "    for text, annotations in data:\n",
        "        entities = annotations['entities']\n",
        "        valid_entities = []\n",
        "        for start, end, label in entities:\n",
        "            valid_start = start\n",
        "            valid_end = end\n",
        "            while valid_start < len(text) and invalid_span_tokens.match(\n",
        "                    text[valid_start]):\n",
        "                valid_start += 1\n",
        "            while valid_end > 1 and invalid_span_tokens.match(\n",
        "                    text[valid_end - 1]):\n",
        "                valid_end -= 1\n",
        "            valid_entities.append([valid_start, valid_end, label])\n",
        "        cleaned_data.append([text, {'entities': valid_entities}])\n",
        "    return cleaned_data"
      ],
      "metadata": {
        "id": "AwOYJumS2-b2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = trim_entity_spans(convert_dataturks_to_spacy(dataturks_JSON_FilePath))"
      ],
      "metadata": {
        "id": "z9gj_0RE3AbJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNAgySI-p3w2",
        "outputId": "60fadf00-ade0-4daa-b040-86e718fb876b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\",\n",
              " {'entities': [[1296, 1622, 'Skills'],\n",
              "   [993, 1154, 'Skills'],\n",
              "   [939, 957, 'College Name'],\n",
              "   [883, 905, 'College Name'],\n",
              "   [856, 860, 'Graduation Year'],\n",
              "   [771, 814, 'College Name'],\n",
              "   [727, 769, 'Designation'],\n",
              "   [407, 416, 'Companies worked at'],\n",
              "   [372, 405, 'Designation'],\n",
              "   [95, 145, 'Email Address'],\n",
              "   [60, 69, 'Location'],\n",
              "   [49, 58, 'Companies worked at'],\n",
              "   [13, 46, 'Designation'],\n",
              "   [0, 12, 'Name']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy==2.1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkRvbRgYcGyv",
        "outputId": "92756187-1f51-4d02-b6a6-2cdc3eb2816a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.1.4\n",
            "  Downloading spacy-2.1.4-cp37-cp37m-manylinux1_x86_64.whl (29.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (0.10.1)\n",
            "Collecting preshed<2.1.0,>=2.0.1\n",
            "  Downloading preshed-2.0.1-cp37-cp37m-manylinux1_x86_64.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 441 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.21.6)\n",
            "Collecting thinc<7.1.0,>=7.0.2\n",
            "  Downloading thinc-7.0.8-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (1.0.9)\n",
            "Collecting blis<0.3.0,>=0.2.2\n",
            "  Downloading blis-0.2.4-cp37-cp37m-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 69.4 MB/s \n",
            "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5\n",
            "  Downloading srsly-1.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 76.7 MB/s \n",
            "\u001b[?25hCollecting plac<1.0.0,>=0.9.6\n",
            "  Downloading plac-0.9.6-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.23.0)\n",
            "Collecting jsonschema<3.1.0,>=2.6.0\n",
            "  Downloading jsonschema-3.0.2-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.1.4) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (22.1.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (57.4.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema<3.1.0,>=2.6.0->spacy==2.1.4) (0.19.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.4) (2.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.4) (4.64.1)\n",
            "Installing collected packages: srsly, preshed, plac, blis, thinc, jsonschema, spacy\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.5\n",
            "    Uninstalling srsly-2.4.5:\n",
            "      Successfully uninstalled srsly-2.4.5\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.8\n",
            "    Uninstalling preshed-3.0.8:\n",
            "      Successfully uninstalled preshed-3.0.8\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 0.7.9\n",
            "    Uninstalling blis-0.7.9:\n",
            "      Successfully uninstalled blis-0.7.9\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.1.5\n",
            "    Uninstalling thinc-8.1.5:\n",
            "      Successfully uninstalled thinc-8.1.5\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.4.2\n",
            "    Uninstalling spacy-3.4.2:\n",
            "      Successfully uninstalled spacy-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 2.1.4 which is incompatible.\n",
            "confection 0.0.3 requires srsly<3.0.0,>=2.4.0, but you have srsly 1.0.6 which is incompatible.\u001b[0m\n",
            "Successfully installed blis-0.2.4 jsonschema-3.0.2 plac-0.9.6 preshed-2.0.1 spacy-2.1.4 srsly-1.0.6 thinc-7.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Dataset**"
      ],
      "metadata": {
        "id": "z2HxexAVhaG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "\n",
        "def train_test_split(data, test_size, random_state):\n",
        "\n",
        "    random.Random(random_state).shuffle(data)\n",
        "    test_idx = len(data) - math.floor(test_size * len(data))\n",
        "    train_set = data[0: test_idx]\n",
        "    test_set = data[test_idx: ]\n",
        "\n",
        "    return train_set, test_set\n",
        "\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size = 0.1, random_state = 42)\n"
      ],
      "metadata": {
        "id": "z67Cy94GdHvl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training SpaCy**"
      ],
      "metadata": {
        "id": "28uDZDq5ht5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "def train_spacy():\n",
        "    \n",
        "    nlp = spacy.blank('en')  # create blank Language class\n",
        "    # create the built-in pipeline components and add them to the pipeline\n",
        "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
        "    if 'ner' not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe('ner')\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "        \n",
        "    # add labels\n",
        "    for _, annotations in train_data:\n",
        "         for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "            \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(10):\n",
        "            print(\"Statring iteration \" + str(itn))\n",
        "            random.shuffle(train_data)\n",
        "            losses = {}\n",
        "            for text, annotations in train_data:\n",
        "                nlp.update(\n",
        "                    [text],  # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,  # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "            print(losses)\n",
        "    return nlp"
      ],
      "metadata": {
        "id": "VXZxVe0Sc7vh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = train_spacy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09D-rkVYdA48",
        "outputId": "d0e26bad-201a-4df4-e97f-52c7ecd9f6f5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statring iteration 0\n",
            "{'ner': 24234.051309814662}\n",
            "Statring iteration 1\n",
            "{'ner': 19230.048606677243}\n",
            "Statring iteration 2\n",
            "{'ner': 14368.234976497673}\n",
            "Statring iteration 3\n",
            "{'ner': 13835.972596655893}\n",
            "Statring iteration 4\n",
            "{'ner': 12513.459990561973}\n",
            "Statring iteration 5\n",
            "{'ner': 10841.755567976837}\n",
            "Statring iteration 6\n",
            "{'ner': 10298.993076364999}\n",
            "Statring iteration 7\n",
            "{'ner': 9706.967570654651}\n",
            "Statring iteration 8\n",
            "{'ner': 9387.009082304934}\n",
            "Statring iteration 9\n",
            "{'ner': 9948.62753915907}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "qY_qk0Y9h1Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.gold import GoldParse\n",
        "from itertools import groupby\n",
        "\n",
        "def doc_to_bilou(nlp, text):\n",
        "    \n",
        "    doc = nlp(text)\n",
        "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
        "    entities = []\n",
        "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
        "        if not entity:\n",
        "            continue\n",
        "        group = list(group)\n",
        "        _, start, _ = group[0]\n",
        "        word, last, _ = group[-1]\n",
        "        end = last + len(word)\n",
        "        \n",
        "        entities.append((\n",
        "                start,\n",
        "                end,\n",
        "                entity\n",
        "            ))\n",
        "\n",
        "    gold = GoldParse(nlp(text), entities = entities)\n",
        "    pred_ents = gold.ner\n",
        "    \n",
        "    return pred_ents\n",
        "\n",
        "y_test = []\n",
        "y_pred = []\n",
        "\n",
        "for text, annots in test_data:\n",
        "    \n",
        "    gold = GoldParse(nlp.make_doc(text), entities = annots.get(\"entities\"))\n",
        "    ents = gold.ner\n",
        "    pred_ents = doc_to_bilou(nlp, text)\n",
        "    \n",
        "    y_test.append(ents)\n",
        "    y_pred.append(pred_ents)\n",
        "    \n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from itertools import chain\n",
        "\n",
        "def ner_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Classification report for a list of BIO-encoded sequences.\n",
        "    It computes token-level metrics and discards \"O\" labels.\n",
        "    \n",
        "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
        "    to calculate averages properly!\n",
        "    \"\"\"\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_)\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset\n",
        "    ), accuracy_score(y_true_combined, y_pred_combined)\n",
        "    \n",
        "report, accuracy = ner_report(y_test, y_pred)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "K24rnyoIeXRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6187862-5fcb-4af1-b36a-20f25b43e120"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "                    -       0.00      0.00      0.00       142\n",
            "       B-College Name       0.79      0.84      0.82        32\n",
            "       I-College Name       0.73      0.87      0.80        63\n",
            "       L-College Name       0.74      0.78      0.76        32\n",
            "       U-College Name       0.00      0.00      0.00         1\n",
            "B-Companies worked at       0.59      0.43      0.50        30\n",
            "I-Companies worked at       0.20      0.25      0.22         4\n",
            "L-Companies worked at       0.64      0.45      0.53        31\n",
            "U-Companies worked at       0.50      0.20      0.29        45\n",
            "             B-Degree       0.87      0.83      0.85        24\n",
            "             I-Degree       0.97      0.92      0.95        66\n",
            "             L-Degree       0.87      0.83      0.85        24\n",
            "             U-Degree       0.50      0.67      0.57         3\n",
            "        B-Designation       0.83      0.53      0.65        45\n",
            "        I-Designation       0.95      0.50      0.66        40\n",
            "        L-Designation       0.83      0.51      0.63        47\n",
            "        U-Designation       0.00      0.00      0.00         1\n",
            "      B-Email Address       1.00      0.86      0.92         7\n",
            "      I-Email Address       1.00      1.00      1.00        20\n",
            "      L-Email Address       1.00      0.86      0.92         7\n",
            "      U-Email Address       0.67      1.00      0.80        10\n",
            "    U-Graduation Year       0.70      0.32      0.44        22\n",
            "           B-Location       1.00      0.33      0.50         3\n",
            "           L-Location       1.00      0.33      0.50         3\n",
            "           U-Location       0.85      0.34      0.49        32\n",
            "               B-Name       0.91      0.87      0.89        23\n",
            "               L-Name       0.91      0.87      0.89        23\n",
            "                    O       0.93      0.99      0.96     12457\n",
            "             B-Skills       0.73      0.42      0.54        26\n",
            "             I-Skills       0.79      0.38      0.52      1022\n",
            "             L-Skills       0.73      0.41      0.52        27\n",
            "             U-Skills       0.00      0.00      0.00         2\n",
            "B-Years of Experience       0.50      0.25      0.33         4\n",
            "L-Years of Experience       0.50      0.25      0.33         4\n",
            "U-Years of Experience       0.00      0.00      0.00         1\n",
            "\n",
            "            micro avg       0.92      0.92      0.92     14323\n",
            "            macro avg       0.66      0.52      0.56     14323\n",
            "         weighted avg       0.90      0.92      0.90     14323\n",
            "          samples avg       0.92      0.92      0.92     14323\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "id": "ymC70Ipzef1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a6e772-321c-4b99-fff0-012d7f648f2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9180339314389444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval==0.0.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1Sg_jeH92lG",
        "outputId": "a56e6df8-3d04-4dd1-e71b-b84549a92456"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval==0.0.12\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12) (1.21.6)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.12) (2.9.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7434 sha256=6596182dbbf1764ec0bd0aad0fee3ec9a454c88c3777b63d31aa468342e8932e\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/cc/62/a3b81f92d35a80e39eb9b2a9d8b31abac54c02b21b2d466edc\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report"
      ],
      "metadata": {
        "id": "nMhVRpki99nU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred,digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB-_DZgO9_p2",
        "outputId": "235258e7-ad8d-48dd-cbbc-cdd763a4b39a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            "             Skills     0.6667    0.3448    0.4545        29\n",
            "           Location     0.8571    0.3429    0.4898        35\n",
            "        Designation     0.8000    0.5000    0.6154        48\n",
            "      Email Address     0.7619    0.9412    0.8421        17\n",
            "               Name     0.8636    0.8636    0.8636        22\n",
            "             Degree     0.8148    0.8148    0.8148        27\n",
            "Companies worked at     0.5750    0.3067    0.4000        75\n",
            "    Graduation Year     0.7000    0.3182    0.4375        22\n",
            "       College Name     0.7143    0.7576    0.7353        33\n",
            "                        0.0000    0.0000    0.0000        12\n",
            "Years of Experience     0.5000    0.2000    0.2857         5\n",
            "\n",
            "          micro avg     0.7361    0.4892    0.5878       325\n",
            "          macro avg     0.6963    0.4892    0.5554       325\n",
            "\n"
          ]
        }
      ]
    }
  ]
}